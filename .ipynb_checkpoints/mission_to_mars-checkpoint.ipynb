{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[//]: # (# <font color=lightblue>P</font><font color=maroon>r</font><font color=teal>o</font><font color=pink>j</font><font color=gold>e</font><font color=lightblue>c</font><font color=maroon>t</font> <font color=teal>G</font><font color=pink>o</font><font color=gold>a</font><font color=lightblue>l</font>)\n",
    "# Project Goal\n",
    "To build a web application that scrapes various websites for data related to the Mission to Mars and displays the information in a single HTML page.\n",
    "***\n",
    "## Step 1 - Scraping\n",
    "\n",
    "Initial scraping and analysis will be completed using Jupyter Notebook, BeautifulSoup, Pandas, and Requests/Splinter.\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dependencies\n",
    "import requests\n",
    "import pandas as pd\n",
    "import time\n",
    "from splinter import Browser\n",
    "from bs4 import BeautifulSoup as bs\n",
    "\n",
    "executable_path = {'executable_path': '/usr/local/bin/chromedriver'}\n",
    "browser = Browser('chrome', **executable_path, headless=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NASA Mars News\n",
    "Scrape the [NASA Mars News Site](https://mars.nasa.gov/news/?page=0&per_page=40&order=publish_date+desc%2Ccreated_at+desc&search=&category=19%2C165%2C184%2C204&blank_scope=Latest) and collect the latest News Title and Paragraph Text. Assign the text to variables for reference later.\n",
    "\n",
    "###### Example:\n",
    "`news_title = \"NASA's Next Mars Mission to Investigate Interior of Red Planet\"`\n",
    "\n",
    "`news_p = \"Preparation of NASA's next spacecraft to Mars, InSight, has ramped up this summer, on course for launch next May from Vandenberg Air Force Base in central California -- the first interplanetary launch in history from America's West Coast.\"`\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# URL of page to be scraped\n",
    "news_url = 'https://mars.nasa.gov/news/'\n",
    "\n",
    "# Retrieve page with the splinter/browser module\n",
    "browser.visit(news_url)\n",
    "\n",
    "# Create BeautifulSoup object; parse with 'html.parser'\n",
    "news_html = browser.html\n",
    "news_soup = bs(news_html, 'lxml')\n",
    "\n",
    "# Check to make sure news_soup is type BeautifulSoup\n",
    "# type(news_soup)\n",
    "\n",
    "# Examine the results, then determine element that contains sought info\n",
    "# print(news_soup.prettify())\n",
    "\n",
    "# Extract latest news article title and description, and save both into variables\n",
    "news_title = news_soup.find('div', class_=\"content_title\").text\n",
    "news_p = news_soup.find('div', class_=\"rollover_description_inner\").text\n",
    "\n",
    "# Check results\n",
    "# print(news_title)\n",
    "# print(news_p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### JPL Mars Space Images - Featured Image\n",
    "Visit the url for JPL Featured Space Image [here](https://www.jpl.nasa.gov/spaceimages/?search=&category=Mars).\n",
    "Use splinter to navigate the site and find the image url for the current Featured Mars Image and assign the url string to a variable called featured_image_url.\n",
    "Make sure to find the image url to the full size .jpg image.\n",
    "Make sure to save a complete url string for this image.\n",
    "\n",
    "###### Example:\n",
    "`featured_image_url = 'https://www.jpl.nasa.gov/spaceimages/images/largesize/PIA16225_hires.jpg'`\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# URL of page to be scraped\n",
    "image_url = 'https://www.jpl.nasa.gov/spaceimages/?search=&category=Mars'\n",
    "\n",
    "# Retrieve page with the splinter/browser module\n",
    "browser.visit(image_url)\n",
    "time.sleep(2)\n",
    "browser.click_link_by_partial_text(\"FULL IMAGE\")\n",
    "time.sleep(2)\n",
    "browser.click_link_by_partial_text(\"more info\")\n",
    "time.sleep(2)\n",
    "\n",
    "# Create BeautifulSoup object; parse with 'html.parser'\n",
    "image_html = browser.html\n",
    "image_soup = bs(image_html, 'lxml')\n",
    "\n",
    "# Check to make sure image_soup is type BeautifulSoup\n",
    "# type(image_soup)\n",
    "\n",
    "# Examine the results, then determine element that contains sought info\n",
    "# print(image_soup.prettify())\n",
    "\n",
    "\n",
    "# Extract featured image url and save into variable\n",
    "buttons = image_soup.find_all('div', class_='download_tiff')\n",
    "for button in buttons:\n",
    "    if ('JPG' in button.text):\n",
    "        featured_image_url = 'https:' + button.a['href']\n",
    "\n",
    "# Check results\n",
    "# print(featured_image_url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mars Weather\n",
    "Visit the Mars Weather twitter account [here](https://twitter.com/marswxreport?lang=en) and scrape the latest Mars weather tweet from the page. Save the tweet text for the weather report as a variable called mars_weather.\n",
    "\n",
    "###### Example:\n",
    "`mars_weather = 'Sol 1801 (Aug 30, 2017), Sunny, high -21C/-5F, low -80C/-112F, pressure at 8.82 hPa, daylight 06:09-17:55'`\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# URL of page to be scraped\n",
    "twitter_url = 'https://twitter.com/marswxreport?lang=en'\n",
    "\n",
    "# Retrieve page with the splinter/browser module\n",
    "browser.visit(twitter_url)\n",
    "\n",
    "# Create BeautifulSoup object; parse with 'html.parser'\n",
    "twitter_html = browser.html\n",
    "twitter_soup = bs(twitter_html, 'lxml')\n",
    "\n",
    "# Check to make sure twitter_soup is type BeautifulSoup\n",
    "# type(twitter_soup)\n",
    "\n",
    "# Examine the results, then determine element that contains sought info\n",
    "# print(twitter_soup.prettify())\n",
    "\n",
    "# Extract latest Mars weather tweet text, and save it into a variable\n",
    "tweets = twitter_soup.find_all('p', class_='tweet-text')\n",
    "# Some tweets are not weather updates, or contain more than one text section\n",
    "for tweet in tweets:\n",
    "    if tweet.text.startswith('InSight'):\n",
    "        tweet_image_text_length = len(tweet.find('a', class_=\"twitter-timeline-link\").text)\n",
    "        mars_weather = tweet.text[:-tweet_image_text_length]\n",
    "        break\n",
    "    else:\n",
    "        continue\n",
    "\n",
    "# Check results\n",
    "# print(mars_weather)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mars Facts\n",
    "Visit the Mars Facts webpage [here](https://space-facts.com/mars/) and use Pandas to scrape the table containing facts about the planet including Diameter, Mass, etc.\n",
    "Use Pandas to convert the data to a HTML table string.\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# URL of page to be scraped\n",
    "facts_url = 'https://space-facts.com/mars/'\n",
    "\n",
    "# Retrieve table with pandas 'read_html'\n",
    "facts_table = pd.read_html(facts_url)\n",
    "mars_table = facts_table[0]\n",
    "\n",
    "# View table\n",
    "# mars_table\n",
    "\n",
    "# Convert table to html object\n",
    "mars_table_html = mars_table.to_html(header=False, index=False)\n",
    "\n",
    "# Check results\n",
    "# print(mars_table_html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mars Hemispheres\n",
    "Visit the USGS Astrogeology site [here](https://astrogeology.usgs.gov/search/results?q=hemisphere+enhanced&k1=target&v1=Mars) to obtain high resolution images for each of Mar's hemispheres.\n",
    "Will need to click each of the links to the hemispheres in order to find the image url to the full resolution image.\n",
    "Save both the image url string for the full resolution hemisphere image, and the Hemisphere title containing the hemisphere name. Use a Python dictionary to store the data using the keys img_url and title.\n",
    "Append the dictionary with the image url string and the hemisphere title to a list. This list will contain one dictionary for each hemisphere.\n",
    "\n",
    "\n",
    "###### Example:\n",
    "`hemisphere_image_urls = [\n",
    "    {\"title\": \"Valles Marineris Hemisphere\", \"img_url\": \"...\"},\n",
    "    {\"title\": \"Cerberus Hemisphere\", \"img_url\": \"...\"},\n",
    "    {\"title\": \"Schiaparelli Hemisphere\", \"img_url\": \"...\"},\n",
    "    {\"title\": \"Syrtis Major Hemisphere\", \"img_url\": \"...\"},\n",
    "]`\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# URL of page to be scraped\n",
    "hemis_url = 'https://astrogeology.usgs.gov/search/results?q=hemisphere+enhanced&k1=target&v1=Mars'\n",
    "\n",
    "# Retrieve page with the splinter/browser module\n",
    "browser.visit(hemis_url)\n",
    "\n",
    "# Create BeautifulSoup object; parse with 'html.parser'\n",
    "hemis_html = browser.html\n",
    "hemis_soup = bs(hemis_html, 'lxml')\n",
    "\n",
    "# Check to make sure twitter_soup is type BeautifulSoup\n",
    "# type(hemis_soup)\n",
    "\n",
    "# Examine the results, then determine element that contains sought info\n",
    "# print(hemis_soup.prettify())\n",
    "\n",
    "# Extract titles and image urls of Mars hemispheres, and save them into list of dictionaries\n",
    "# Create empty list\n",
    "hemisphere_image_urls = []\n",
    "hemispheres = hemis_soup.find_all('div', class_=\"item\")\n",
    "\n",
    "# Iterate through hemispheres\n",
    "for hemis in hemispheres:\n",
    "    # Create empty dictionary\n",
    "    hemis_info = {}\n",
    "    # Find title \n",
    "    hemis_title = hemis.div.a.h3.text\n",
    "    hemis_info['title'] = hemis_title\n",
    "    \n",
    "    # Find image url by clicking on the title and then finding linked text to sample jpeg image\n",
    "    browser.click_link_by_partial_text(hemis_title)\n",
    "    time.sleep(1)\n",
    "    hemis_img_html = browser.html\n",
    "    hemis_img_soup = bs(hemis_img_html, 'lxml')\n",
    "    hemis_img = hemis_img_soup.find('div', class_='downloads').ul.li.a['href']\n",
    "    hemis_info['img_url'] = hemis_img\n",
    "    \n",
    "    # Add title and url dictionary to list of dictionaries\n",
    "    hemisphere_image_urls.append(hemis_info)\n",
    "    \n",
    "    # Go back to main page in preparation for next img search\n",
    "    browser.back()\n",
    "    time.sleep(1)\n",
    "    \n",
    "# Check results\n",
    "# print(hemisphere_image_urls)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
